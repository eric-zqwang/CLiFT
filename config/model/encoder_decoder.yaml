model_name:
  _target_: src.model.encoder_decoder.LiFTnvs

patch_size: 8
token_ratio: random

encoder:
  hidden_dim: 768
  num_layers: 6
  num_attention_heads: 8
  lr_scale: 1.0

decoder:
  hidden_dim: 768
  num_layers: 6
  num_attention_heads: 8
  lr_scale: 1.0

loss:
  perceptual_weight: 0.5

optimizer:
  _target_: torch.optim.AdamW
  lr: 4e-4
  betas: [0.9, 0.95]
  weight_decay: 0.05
  eps: 1e-8

lr_scheduler:
  _target_: src.utils.scheduler.CosineWarmupScheduler
  warmup_iters: 2500
  initial_lr: 1e-8

squeezer:
  num_layers: 2
  decoder_lr_scale: 0.1

ckpt_path: null

trainable_modules: null
pretrained_modules: null

kmeans: sklearn