model_name:
  _target_: src.model.lvsm_encoder_decoder.Transformer

patch_size: 8

encoder:
  hidden_dim: 512
  num_layers: 6
  num_attention_heads: 8
  lr_scale: 1.0

decoder:
  hidden_dim: 512
  num_layers: 6
  num_attention_heads: 8
  lr_scale: 1.0

loss:
  perceptual_weight: 0.5

optimizer:
  _target_: torch.optim.AdamW
  lr: 4e-4
  betas: [0.9, 0.95]
  weight_decay: 0.05
  eps: 1e-8

lr_scheduler:
  _target_: src.utils.scheduler.CosineWarmupScheduler
  warmup_iters: 2500
  initial_lr: 1e-8

ckpt_path: null

trainable_modules: null
pretrained_modules: null